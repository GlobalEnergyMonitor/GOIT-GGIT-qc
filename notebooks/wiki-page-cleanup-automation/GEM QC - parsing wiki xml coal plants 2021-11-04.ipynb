{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84858f4c-d698-4d73-a78b-d48918cc93db",
   "metadata": {},
   "source": [
    "# TO DO:\n",
    "* Figure out how to parse pages with separate project details sections, and with multiple units on different lines within a project details section\n",
    "  * Example: https://www.gem.wiki/Qiaotou_power_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f115071-f6bb-4785-889e-04fb4a5bd027",
   "metadata": {},
   "source": [
    "# compare working spreadsheet against wiki (not finalized spreadsheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b45cf-e66d-4a77-a6ef-e1423f8ec7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd163ea-c59f-4b81-8e5e-13c191855d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.parse\n",
    "from urllib.parse import unquote\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b11bb1-343a-43a0-8dd8-0f6f425d14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzywuzzy as fuzz\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36d7b994-97b2-448c-95fe-97c3b2d95126",
   "metadata": {},
   "source": [
    "gcpt_path = '/Users/masoninman/Dropbox/GEM/coal plants (GCPT)/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72119d1-155e-4295-b795-e74e3e29767a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import spreadsheet data (working version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86988a-0028-47c7-ba8c-7387eb6380e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcpt_file = 'Global Coal Plant Tracker Files 2021-12-09_1427 - compiled.xlsx'    \n",
    "gcpt = pd.read_excel(\n",
    "    gcpt_path + gcpt_file, \n",
    "    sheet_name='Sheet1', \n",
    "    dtype={'Wiki page': str}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594422f-e87f-43ed-9d68-fe387f5bf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: delete if covered by cell 2 below\n",
    "# # create column 'Wiki title' in the df\n",
    "\n",
    "# # TO DO: need to also convert special characters (such as accented letters)\n",
    "# # (see code in Latin America Portal map data compiling 2021-11-02.ipynb)\n",
    "\n",
    "# # TO DO: add step to compare, within the spreadsheet, the wiki page url/title and the plant name\n",
    "# # Some mismatches may be due to old URLs that now redirect\n",
    "\n",
    "# wiki_str = gcpt['Wiki page']\n",
    "# wiki_str = wiki_str.str.replace('https://gem.wiki/', 'https://www.gem.wiki/', regex=False)\n",
    "# wiki_str = wiki_str.str.split('https://www.gem.wiki/')\n",
    "# wiki_str = wiki_str.str[-1].str.replace('_', ' ')\n",
    "# gcpt['Wiki title'] = wiki_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89b0b8-c7d2-42c7-bc0b-d43c07611f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: delete this cell?\n",
    "\n",
    "# titles_to_process = df['title'].unique().tolist()\n",
    "\n",
    "# # exclude template pages\n",
    "# titles_to_process = [x for x in titles_to_process if x.endswith('template page') == False]\n",
    "\n",
    "# # clean up names\n",
    "# titles_to_process = [x.replace('&amp;', '&') for x in titles_to_process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0978f-a69a-44db-8b75-7a1307680deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wiki page titles from URLs\n",
    "spreadsheet = gcpt.copy()\n",
    "spreadsheet['Wiki page'] = spreadsheet['Wiki page'].astype(str)\n",
    "\n",
    "unexpected_count = 0 # initialize\n",
    "for row in spreadsheet.index:\n",
    "    wiki = spreadsheet.at[row, 'Wiki page']\n",
    "    plant = spreadsheet.at[row, 'Plant']\n",
    "\n",
    "    if wiki.startswith('http') and 'gem.wiki' in wiki:\n",
    "        # convert wiki urls to wiki page names\n",
    "        w = unquote(wiki)\n",
    "        w = w.replace('_', ' ')\n",
    "        w = w.split('gem.wiki/')[-1]\n",
    "        \n",
    "        # put value into spreadsheet\n",
    "        spreadsheet.at[row, 'Wiki title'] = w\n",
    "    else:\n",
    "        # print('Error!' + f\" Unexpected format for wiki URL for {plant}: {wiki}\")\n",
    "        print(f\"{plant}: {wiki}\")\n",
    "        unexpected_count += 1\n",
    "        \n",
    "print(unexpected_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aa2205-8556-4775-80bb-0746bec6a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export wiki titles to excel\n",
    "# # note that some of the rows don't have a wiki title (perhaps because of links above to sites other than GEM)\n",
    "\n",
    "# wiki_titles_file_name = gcpt_file.replace('compiled.xlsx', 'wiki titles.xlsx')\n",
    "# spreadsheet['Wiki title'].sort_values().to_excel(wiki_titles_file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19c00f0-e8a6-4238-b51c-b4edecfe8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to fit with wiki pages:\n",
    "spreadsheet = spreadsheet.rename(columns={'Year': 'Start year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f7c4c-e1c5-4318-89f2-73112620044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet[spreadsheet['Plant'].str.contains('  ')]['Plant'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d1eab-9946-4074-adc7-b7cbecbbd143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox:\n",
    "test = spreadsheet[spreadsheet['Wiki page']=='nan']['Plant'].drop_duplicates()\n",
    "for row in test.index:\n",
    "    print(test.at[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd0e23-bdd6-46e6-a4d5-bf2033c03d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox: how often are there more than one set of coordinates for a given plant?\n",
    "unit_count = spreadsheet.groupby('Plant')['Plant'].count()\n",
    "unit_multi = unit_count[unit_count > 1]\n",
    "num_plants_multi_unit = len((unit_multi.index.unique()))\n",
    "\n",
    "lat_count = spreadsheet[['Plant', 'Latitude']].drop_duplicates().groupby('Plant')['Latitude'].count()\n",
    "\n",
    "lat_multi = lat_count[lat_count > 1]\n",
    "print(f\"Number of plants with more than one set of coordinates: {len(lat_multi)} (of {num_plants_multi_unit} plants with more than one unit).\")\n",
    "\n",
    "# show:\n",
    "spreadsheet[spreadsheet['Plant'].isin(lat_multi.index)].sort_values(by=['Plant', 'Unit'])[['Plant', 'Unit', 'Latitude', 'Longitude']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37642e1-bb32-43bc-a29a-bf493ff0e64f",
   "metadata": {},
   "source": [
    "## Wiki XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798049b-63ca-4f7f-9c40-72ac948208e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define wiki file (XML)\n",
    "\n",
    "# # china_file = 'GEM wiki export - Coal plants in China - 20211104105337.xml'\n",
    "# china_file = 'GEM wiki export - Existing coal plants in China - 20211104131345.xml'\n",
    "# china_path = '/Users/masoninman/Downloads/'\n",
    "\n",
    "xml_file = 'GEM wiki export 2021-12-09_190304 - coal plants (all in working spreadsheet).xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b19322-900e-4a6b-a1e9-c39a52217b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIKI: read in whole XML file\n",
    "with open(gcpt_path + xml_file) as f: data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdd7a2-76fc-475f-8dc1-9ee1fd1837d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_counter = 0 # initialize\n",
    "page_line_numbers = []\n",
    "for line in data:\n",
    "    if line=='  <page>\\n':\n",
    "        page_line_numbers += [line_counter]\n",
    "    line_counter += 1\n",
    "    \n",
    "line_counter = 0 # initialize\n",
    "end_page_line_numbers = []\n",
    "for line in data:\n",
    "    if line=='  </page>\\n':\n",
    "        end_page_line_numbers += [line_counter]\n",
    "    line_counter += 1\n",
    "    \n",
    "print(f\"Number of page lines: {len(page_line_numbers)}\")\n",
    "\n",
    "all_pages = []\n",
    "if len(page_line_numbers) == len(end_page_line_numbers):\n",
    "    for num in range(len(page_line_numbers)):\n",
    "        # get the line numbers for start and end of page\n",
    "        range_of_lines_to_group = range(\n",
    "            page_line_numbers[num], \n",
    "            end_page_line_numbers[num]+1)\n",
    "\n",
    "        page_strs = []\n",
    "        for line in range_of_lines_to_group:\n",
    "            page_strs += [data[line]]\n",
    "            \n",
    "        all_pages += [page_strs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b6891-31f0-447a-8f75-d569ac0fed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_redirects(all_pages):\n",
    "    redirects_list = [] # initialize\n",
    "    for page_num in range(len(all_pages)):\n",
    "        page_list = all_pages[page_num]\n",
    "        for line in page_list:\n",
    "            title = '' # initialize\n",
    "            redirect_title = line.replace('<redirect title=', '').replace(' />', '').strip('/n').strip().strip('\"')\n",
    "            if '<redirect title' in line:\n",
    "                # get title line also:\n",
    "                for other_line in page_list:\n",
    "                    if '<title>' in other_line:\n",
    "                        title = other_line.replace('<title>', '').replace('</title>', '').strip('/n').strip()\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                redirects_list += [(title, redirect_title)]\n",
    "            else:\n",
    "                # no redirect\n",
    "                pass\n",
    "\n",
    "    redirects_df = pd.DataFrame(redirects_list, columns=['Wiki title', 'Wiki redirect title'])\n",
    "    return redirects_df\n",
    "\n",
    "redirects_df = find_redirects(all_pages)\n",
    "\n",
    "redirects_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5ef70-3bbf-4ed5-8109-81637b7b145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mismatches_plant_name_and_wiki_title(spreadsheet, redirects_df):\n",
    "    \"\"\"\n",
    "    Uses redirects established above to see what the final wiki page title is.\n",
    "    \"\"\"\n",
    "\n",
    "    test = spreadsheet.copy()[['Plant', 'Wiki title']].drop_duplicates()\n",
    "\n",
    "    # merge in redirects\n",
    "    test = pd.merge(\n",
    "        test,\n",
    "        redirects_df,\n",
    "        on='Wiki title', \n",
    "        how='left')\n",
    "\n",
    "    for row in test.index:\n",
    "        if pd.isna(test.at[row, 'Wiki redirect title']):\n",
    "            test.at[row, 'Wiki redirect title'] = test.at[row, 'Wiki title']\n",
    "\n",
    "    test = test.rename(columns={'Wiki redirect title': 'Wiki title (after redirects)'})\n",
    "\n",
    "    # for this first comparison, ignore capitalization\n",
    "    test = test[\n",
    "        test['Wiki title (after redirects)'].str.strip().str.lower()!=\n",
    "        test['Plant'].str.strip().str.lower()\n",
    "    ][['Plant', 'Wiki title (after redirects)']]\n",
    "    \n",
    "    test = test.sort_values(by=['Plant', 'Wiki title (after redirects)'])\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    # exclude those with missing wiki title\n",
    "    test = test[test['Wiki title (after redirects)'].isna()==False]\n",
    "    \n",
    "    return test\n",
    "\n",
    "test = mismatches_plant_name_and_wiki_title(spreadsheet, redirects_df)\n",
    "# test[test['Wiki title (after redirects)'].str.contains('redirect', na=False)].reset_index(drop=True)\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc566585-cb7c-43b4-9668-1e3598e0cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sandbox:\n",
    "# find plants with more than one sponsor\n",
    "test = spreadsheet.copy()\n",
    "test = test[['Plant', 'Sponsor']].drop_duplicates()\n",
    "sponsor_count = test.groupby('Plant')['Sponsor'].count()\n",
    "sponsor_multi = sponsor_count[sponsor_count > 1]\n",
    "\n",
    "spreadsheet[spreadsheet['Plant'].isin(sponsor_multi.index)][['Plant', 'Unit', 'Status', 'Sponsor', 'Parent']].sort_values(by=['Plant', 'Unit']).reset_index(drop=True).loc[61:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f538d6d-cffb-4215-ac2f-9a1bdd284c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spreadsheet[spreadsheet['Plant'].str.contains('Blachownia power station')].sort_values(by='Unit')[['Plant', 'Unit', 'Status', 'Capacity (MW)', 'Sponsor', 'Parent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f79c5-2278-4997-b866-65179c27806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples:\n",
    "# Afşin-Elbistan-A power station\n",
    "# wiki page is Afşin-Elbistan power complex, which includes plants A, B, C\n",
    "# ownership for Afşin-Elbistan A described in narrative section, but there's no mention of any units in plant A being owned by EÜAŞ\n",
    "# Project Details for Afşin-Elbistan B\n",
    "\n",
    "# Asam-Asam power station\n",
    "# Operating units have ownership in narrative section; project details is only for units under construction\n",
    "\n",
    "# Bander power station\n",
    "# Ownership for cancelled unit 3 is not included in wiki page\n",
    "\n",
    "# Banten Suralaya power station:\n",
    "# only project details section is \"Project Details for Units 9 & 10 (1000MW) Expansion\"\n",
    "# background section has details on operating units, and says \"The complex is owned by PT PLN.\" \n",
    "# That agrees with spreadsheet parent, but spreadsheet sponsor isn't included in wiki.\n",
    "\n",
    "# Bau-Bau Power Station\n",
    "# Undetermined. Similar but not exactly the same entries in wiki vs spreadsheet\n",
    "\n",
    "# Beilun power station\n",
    "# Wiki page has complicated entry for Sponsor:\n",
    "# Sponsor: Beijing Guodian Power Co,. Ltd. 北京国电电力有限公司 70% of unit 1&2, 49% of unit 3-5, 50% of unit 6 &7; Zhejiang Zheneng Power Co,. Ltd. 浙江浙能电力股份有限公司 30% of unit 1&2, 51% of unit 3-5, 40% of unit 6&7; Ningbo Thermal Power Co., Ltd.宁波热电股份有限公司 10% of unit 6&7.\n",
    "# Parent entry seems to be just the single controlling parent of each sponsor \n",
    "# Parent: National Energy Investment Group, Zhejiang Provincial Energy Group\n",
    "# wiki doesn't match spreadsheet, which does have parent percentages\n",
    "\n",
    "# Benga power station\n",
    "# Wiki project details section ostensibly is for both units (phase 1 & phase 2),\n",
    "# but it only lists one sponsor/parent pair, which matches spreadsheet phase 1 (not exact string match, but it is match when human reads it)\n",
    "\n",
    "# Bielsko-Biala power station\n",
    "# Wiki has no project details section; no mention of the sponsor for the operating unit (unit EC1)\n",
    "\n",
    "# Blachownia power station: only retired/cancelled; skip?\n",
    "\n",
    "# CPI Dabieshan power station\n",
    "# Project Details for Units 3 & 4 Expansion\n",
    "# Ownership for units 1 & 2 covered by narrative, but not specifically in project details\n",
    "# Wiki page doesn't have a row for Parent--but what is entered as sponsor is same as what spreadsheet has for parent\n",
    "\n",
    "# CPI Pingwei power station\n",
    "# Phase I: covered in \"Plant Details\" section, although it's not clear how the 2 sponsors (with no percentages listed) correspond to the 2 parents (with percentages listed)\n",
    "# doesn't match what's in spreadsheet\n",
    "# Phase II: not covered in any project details section\n",
    "# data is included in spreadsheet\n",
    "# Phase III: covered in section \"Project Details for Phase III Expansion\"\n",
    "# sponsor & parent don't match spreadsheet\n",
    "# Phase IV: covered in section \"Project Details for Phase IV Expansion\"\n",
    "# wiki doesn't list sponsor at all, but it is in spreadsheet; for parent, wiki and spreadsheet don't match\n",
    "\n",
    "# CPI Shanxi Houma power station\n",
    "# There are 6 retired units; all the ownership information in the wiki page matches what the spreadsheet has for these retired units\n",
    "# For the 2 operating units, spreadsheet has different sponsor & owner than stated in the wiki\n",
    "\n",
    "\n",
    "\n",
    "# Duernrohr power station\n",
    "# Wiki page lists: \"Sponsor: Verbund (Unit 1), EVN (Unit 2)\"\n",
    "\n",
    "# Patuakhali power station (Ashuganj)\n",
    "# Wiki page doesn't agree with sheet; wiki page only lists one set of sponsors, for all units: \"Sponsor: Ashuganj Power Station Company Limited (APSCL), China Energy Engineering Corporation\"\n",
    "\n",
    "# Gacko Thermal Power Plant\n",
    "# Wiki page project details is only for the announced unit (excludes the operating unit)\n",
    "\n",
    "# Kakanj Thermal Power Plant\n",
    "# Wiki page has \"Project Details for Unit 8\" but no project details section for other units\n",
    "\n",
    "# Tuzla Thermal Power Plant\n",
    "# Wiki page has section \"Project Details for expansion\" but other units don't have project details sections\n",
    "\n",
    "# Porto do Pecém power station: wiki page has all details laid out with references\n",
    "# Sponsor:\n",
    "#     Unit 1: EDP Energias do Brasil[2]\n",
    "#     Unit 2: EDP Energias do Brasil[2]\n",
    "#     Unit 3: Eneva[14]\n",
    "\n",
    "# Sihanoukville CEL power station:\n",
    "# wiki page has project details section that lists details for all units;\n",
    "# but for sponsor & parent, only has one set of info, which matches the operating unit 1 & 2, but not unit 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3632c-4dcc-4935-9e93-6feba6c0253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# Wabamum Generating Station: Wiki has typo in page name & throughout. Should be \"Wabamun\" (ending with an \"n\").\n",
    "# Angul Steel power station: need to make sponsor names consistent; one has Jindal Steel & Power Ltd and others have Jindal Steel & Power\n",
    "# Asam-Asam power station: unit names in wiki and spreadsheet don't match up; wiki calls those under construction Unit 5 & 6\n",
    "# Belchatow power station: sponsor is written two different ways for different units: PGE Górnictwo i Energetyka Konwencjonalna SA & PGE GiEK\n",
    "# CEEC Chongzuo power station: sponsor is written two different ways; make consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ed7e1-88a8-49e5-85dc-016874f93704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_details_line_num(one_page):\n",
    "    project_details_line_num = '' # initialize\n",
    "    for x in range(len(one_page)):\n",
    "        one_page_line = one_page[x]\n",
    "        if one_page_line.startswith('== '):\n",
    "            one_page_line = one_page_line.replace('== ', '==')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        if (one_page_line.strip().lower().startswith('==project details') | \n",
    "            one_page_line.strip().lower().startswith('==plant details') | \n",
    "            one_page_line.strip().lower().startswith('==plant data')\n",
    "           ):\n",
    "            project_details_line_num = x\n",
    "            break\n",
    "            # TO DO: handle cases with more than one project/plant details section\n",
    "            # e.g., \"Plant Details\" & \"Plant Details of expansion\"\n",
    "\n",
    "    return project_details_line_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ac6cd-7bc4-42be-9cef-6318f0b4aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wiki_df(all_pages):\n",
    "    \n",
    "    # initialize new df\n",
    "    df = pd.DataFrame(columns=['page num', 'title', 'category', 'value'])\n",
    "    \n",
    "    for page_num in range(len(all_pages)):\n",
    "        one_page = all_pages[page_num]\n",
    "\n",
    "        redirect = False # initialize\n",
    "        for x in range(len(one_page)):\n",
    "            if '#REDIRECT' in one_page[x]:\n",
    "                redirect = True\n",
    "\n",
    "        if redirect == False:\n",
    "            for x in range(len(one_page)):\n",
    "                if one_page[x].startswith('    <title>'):\n",
    "                    title_line_num = x\n",
    "                    break\n",
    "\n",
    "            title = one_page[title_line_num].split('<title>')[1].split('</title>')[0]\n",
    "\n",
    "            if title.startswith('Category'):\n",
    "                pass\n",
    "            else:\n",
    "                project_details_line_num = find_project_details_line_num(one_page)\n",
    "\n",
    "                if project_details_line_num == '':\n",
    "                    print(f\"No project details section found for {title} (page_num: {page_num})\")\n",
    "\n",
    "                elif project_details_line_num != '':\n",
    "                    # get line number for next section after project details\n",
    "                    for x in range(project_details_line_num+1, len(one_page)):\n",
    "                        if one_page[x].startswith('=='):\n",
    "                            next_section_line_num = x\n",
    "                            break\n",
    "\n",
    "                    # within project details, find first-level entries (one asterisk)\n",
    "                    for line in range(project_details_line_num+1, next_section_line_num):\n",
    "                        one_page_line = one_page[line]\n",
    "                        # remove spaces after asterisks\n",
    "                        one_page_line = one_page_line.replace('* ', '*')\n",
    "                        # remove line break at end, and strip\n",
    "                        one_page_line = one_page_line.replace('\\n', '').strip()                    \n",
    "\n",
    "                        if one_page_line.startswith(\"*'\") and \"'''\" in one_page_line:\n",
    "                            line_split = one_page_line.split(\":\")\n",
    "                            category = line_split[0].strip(\"*' \")\n",
    "                            # print(category)\n",
    "                            try:\n",
    "                                value = line_split[1].split('\\n')[0].strip(\"*' \")\n",
    "                            except:\n",
    "                                print(f\"For {title}, wasn't able to get value from line: {one_page_line}\")\n",
    "                                value = ''\n",
    "                            # print(value)\n",
    "\n",
    "                            # TO DO: if a line has the status and capacity in it, parse those\n",
    "\n",
    "                            # put into df\n",
    "                            df = df.append(pd.DataFrame({\n",
    "                                'page num': [page_num],\n",
    "                                'title': [title],\n",
    "                                'category': [category],\n",
    "                                'value': [value]}))\n",
    "\n",
    "                        elif one_page_line.startswith(\"'''\"):\n",
    "                            line_split = one_page_line.split(\":\")\n",
    "                            category = line_split[0].strip(\"' \")\n",
    "                            # print(category)\n",
    "                            try:\n",
    "                                value = line_split[1].split('\\n')[0].strip(\"' \")\n",
    "                            except:\n",
    "                                print(f\"For {title}, wasn't able to get value from line: {one_page_line}\")\n",
    "                                value = ''\n",
    "                            # print(value)\n",
    "\n",
    "                            # TO DO: if a line has the status and capacity in it, parse those\n",
    "\n",
    "                            # put into df\n",
    "                            df = df.append(pd.DataFrame({\n",
    "                                'page num': [page_num],\n",
    "                                'title': [title],\n",
    "                                'category': [category],\n",
    "                                'value': [value]}))\n",
    "\n",
    "                        elif one_page_line.startswith(\"**\"):\n",
    "                            # second-level entries\n",
    "                            # for now, don't handle these\n",
    "                            pass\n",
    "\n",
    "                        elif one_page_line == '':\n",
    "                            pass\n",
    "\n",
    "                        else:\n",
    "                            # print(f\"For {title}, unexpected type of line: {one_page_line}\")\n",
    "                            # TO DO: look more into how to get data out of these\n",
    "                            pass\n",
    "                        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0dcc4-e8a1-4494-a507-e22f56078559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wiki_data_df(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    df['value'] = df['value'].str.strip()\n",
    "\n",
    "    # harmonize terms in df\n",
    "    # Sponsor\n",
    "    df['category'] = df['category'].replace({\n",
    "        'Owner': 'Sponsor',\n",
    "        'Sponsor/Owner': 'Sponsor',\n",
    "    })\n",
    "\n",
    "    # Parent\n",
    "    df['category'] = df['category'].replace({\n",
    "        'Parent company': 'Parent',\n",
    "        'Parent Company': 'Parent',\n",
    "    })\n",
    "    # Coal type\n",
    "    df['category'] = df['category'].replace('Coal Type', 'Coal type')\n",
    "    \n",
    "    # Coal source\n",
    "    df['category'] = df['category'].replace('Coal Source', 'Coal source')\n",
    "    \n",
    "    # Start year\n",
    "    df['category'] = df['category'].replace({\n",
    "        'Start Year': 'Start year',\n",
    "        'Start date': 'Start year',\n",
    "        'Projected in service': 'Start year',\n",
    "        # typos:\n",
    "        'Stary year': 'Start year',\n",
    "        'Star year': 'Start year',\n",
    "        # in service\n",
    "        'In service': 'Start year',\n",
    "        'In Service': 'Start year',\n",
    "    })\n",
    "    \n",
    "    # Retired year\n",
    "    df['category'] = df['category'].replace('Retired', 'Retired year')\n",
    "    \n",
    "    # Capacity\n",
    "    df['category'] = df['category'].replace('Gross capacity', 'Capacity')\n",
    "    df['category'] = df['category'].replace('Gross Capacity', 'Capacity')\n",
    "    \n",
    "    # Combustion technology (term in spreadsheet)\n",
    "    df['category'] = df['category'].replace('Type', 'Combustion technology')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f7d49-1409-40e0-bf25-e48018c776d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df = create_wiki_df(all_pages)\n",
    "wiki_df = clean_wiki_data_df(wiki_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cd7266-62b8-4f4f-8cb0-c3785d21ce00",
   "metadata": {},
   "source": [
    "in Existing coal plants in China, there are:\n",
    "\n",
    "Sponsor/owner entries:\n",
    "* Sponsor: 865\n",
    "* Owner: 57\n",
    "* Sponsor/Owner: 15\n",
    "\n",
    "Parent entries:\n",
    "* Parent company: 600\n",
    "* Parent Company: 32\n",
    "* Parent: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138526b-60df-48cb-802f-250b15e05088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sponsor_list = ['Sponsor', 'Owner', 'Sponsor/Owner']\n",
    "# sponsor_values = df[df['category'].isin(sponsor_list)]['value']\n",
    "# sponsor_values = sponsor_values.str.replace('[', '', regex=False).str.replace(']', '', regex=False)\n",
    "# sponsor_values.value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50732a8-685c-402d-a1ef-a2e06d949e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_values(sel_val, sel_col):\n",
    "    \"\"\"\n",
    "    Remove parts of values before comparing wiki vs spreadsheet.\n",
    "    \"\"\"\n",
    "    sel_val = sel_val.strip('. ')\n",
    "    \n",
    "    if sel_col == 'Status':\n",
    "        sel_val = sel_val.replace('Canceled', 'Cancelled')\n",
    "        sel_val = sel_val.replace('canceled', 'Cancelled')\n",
    "\n",
    "    if sel_col in ['Sponsor', 'Parent']:\n",
    "        for search_str in [\n",
    "            'Limited', 'Ltd', \n",
    "            'Corporation', 'Corp', \n",
    "            'Company', 'Co', \n",
    "            'Sa', 'SA'\n",
    "        ]:\n",
    "            if sel_val.endswith(search_str):\n",
    "                sel_val = sel_val[:-len(search_str)]\n",
    "            \n",
    "        sel_val = sel_val.strip('. ')\n",
    "    \n",
    "    if sel_val == 'nan':\n",
    "#         print(\"found a nan!\")\n",
    "        sel_val = ''\n",
    "    \n",
    "    return sel_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6aa87-021c-419f-a68e-489593490e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_wiki_and_spreadsheet_plant_level_data(wiki_df, spreadsheet, sel_cols):\n",
    "    \"\"\"\n",
    "    For each plant in spreadsheet, get values in selected column.\n",
    "    If same for all units in a plant, compare against wiki.\n",
    "    If not the same for all units in a plant, compare one by one against wiki.\n",
    "    Find which unit entries match wiki.\n",
    "    \n",
    "    Only set up to work for Sponsor, Parent.\n",
    "    \"\"\"\n",
    "    \n",
    "    cols_handled = ['Sponsor', 'Parent']\n",
    "    \n",
    "    sel_col_not_plant_level = 'no' # initialize\n",
    "    for sel_col in sel_cols:\n",
    "        if sel_col not in cols_handled:\n",
    "            sel_col_not_plant_level = 'yes'\n",
    "            print(f\"The selected column is not a plant-level characteristic: {sel_col}\")\n",
    "            print(\"The function will only process plants with one unit.\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    results_df_cols = [\n",
    "        'Match status',\n",
    "        'Fuzz partial ratio',\n",
    "        'Wiki title',\n",
    "        'sel_col',\n",
    "        'sel_col - wiki num',\n",
    "        'sel_col - wiki',\n",
    "        'sel_col - spreadsheet'\n",
    "    ]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=results_df_cols)\n",
    "     \n",
    "    if sel_col_not_plant_level == 'yes':\n",
    "        # filter to only keep plants with a single unit\n",
    "        units_per_plant = spreadsheet.groupby('Plant')['Unit'].count()\n",
    "        single_units = units_per_plant[units_per_plant == 1]\n",
    "        spreadsheet = spreadsheet[spreadsheet['Plant'].isin(single_units.index)]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # create list of wiki titles to process\n",
    "    # exclude template pages; clean up names\n",
    "    titles_to_process = wiki_df['title'].unique().tolist()    \n",
    "    titles_to_process = [x for x in titles_to_process if x.endswith('template page') == False]\n",
    "    titles_to_process = [x.replace('&amp;', '&') for x in titles_to_process]\n",
    "    print(f\"Number of titles to process: {len(titles_to_process)}\")\n",
    "\n",
    "    for title_num in range(len(titles_to_process)):\n",
    "        if title_num % 1000 == 0:\n",
    "            print(f\"Processing title_num {title_num}\")\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # select all data for this wiki page\n",
    "        sel_title = titles_to_process[title_num]\n",
    "        sel_wiki = wiki_df[wiki_df['title']==sel_title]\n",
    "\n",
    "        for sel_col in sel_cols:\n",
    "\n",
    "            # get sel_col listed in one wiki page\n",
    "            # TO DO: change to ['value'].unique().tolist(); cut line after with list(set(...))\n",
    "            wiki_sel_col_vals = sel_wiki[sel_wiki['category']==sel_col]['value'].tolist()\n",
    "            wiki_sel_col_vals = list(set(wiki_sel_col_vals))\n",
    "\n",
    "            # clean up to remove references (using list comprehension)\n",
    "            wiki_sel_col_vals = [x.split('&lt;')[0] for x in wiki_sel_col_vals]\n",
    "\n",
    "            # clean up to remove double brackets (for internal wiki links)\n",
    "            wiki_sel_col_vals = [x.replace('[', '').replace(']', '') for x in wiki_sel_col_vals]\n",
    "\n",
    "            # clean up &amp;\n",
    "            wiki_sel_col_vals = [x.replace('&amp;', '&') for x in wiki_sel_col_vals]\n",
    "\n",
    "            # get sel_col listed in spreadsheet for this same wiki page\n",
    "            sheet_sel_col = spreadsheet[spreadsheet['Plant'].str.strip()==sel_title][sel_col]\n",
    "            # TO DO: change to ['value'].unique().tolist(); cut line after with list(set(...))\n",
    "            sheet_sel_col_vals = sheet_sel_col.astype(str).tolist()\n",
    "            sheet_sel_col_vals = list(set(sheet_sel_col_vals))\n",
    "\n",
    "            if len(sheet_sel_col_vals) == 0:\n",
    "                data_dict = {\n",
    "                    'Match status': 'plant not found in spreadsheet',\n",
    "                    'Wiki title': sel_title,\n",
    "                    'sel_col': sel_col,\n",
    "                }\n",
    "                results_df = results_df.append(data_dict, ignore_index=True)\n",
    "\n",
    "            else:\n",
    "                wiki_sel_col_num = 1 # initialize numbering within a given plant\n",
    "                for wiki_sel_col_val in wiki_sel_col_vals:\n",
    "                    # compare wiki against spreadsheet\n",
    "                    for sheet_sel_col_val in sheet_sel_col_vals:\n",
    "\n",
    "                        # if there is a pipe, keep only the part before the pipe\n",
    "                        # example: Aliağa Enka power station has \"[[Enka Enerji|Enka]]\"\n",
    "                        wiki_sel_col_val = wiki_sel_col_val.split('|')[0]\n",
    "\n",
    "                        # clean each of the values:\n",
    "                        wiki_sel_col_val_for_compare = clean_values(wiki_sel_col_val, sel_col)\n",
    "                        sheet_sel_col_val_for_compare = clean_values(sheet_sel_col_val, sel_col)\n",
    "\n",
    "                        if wiki_sel_col_val.lower() == sheet_sel_col_val.lower():\n",
    "                            match_value = 'match'\n",
    "                        else:\n",
    "                            if wiki_sel_col_val_for_compare == sheet_sel_col_val_for_compare:\n",
    "                                match_value = 'close match'\n",
    "                            else:\n",
    "                                match_value = 'mismatch'\n",
    "\n",
    "                        fuzz_partial_ratio = fuzz.partial_ratio(\n",
    "                            str(wiki_sel_col_val_for_compare).lower(), \n",
    "                            str(sheet_sel_col_val_for_compare).lower()\n",
    "                        )\n",
    "                        data_dict = {\n",
    "                            'Match status': match_value,\n",
    "                            'Fuzz partial ratio': fuzz_partial_ratio,\n",
    "                            'Wiki title': sel_title,\n",
    "                            'sel_col': sel_col,\n",
    "                            'sel_col - wiki num': wiki_sel_col_num,\n",
    "                            'sel_col - wiki': wiki_sel_col_val,\n",
    "                            'sel_col - spreadsheet': sheet_sel_col_val,\n",
    "                        }\n",
    "                        results_df = results_df.append(data_dict, ignore_index=True)\n",
    "\n",
    "                    # update wiki_sel_col_num after iterating through sheet_sel_col_vals\n",
    "                    wiki_sel_col_num += 1\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa6be1-a934-47c2-9bfb-f088f7c6078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_status = compare_wiki_and_spreadsheet_plant_level_data(\n",
    "    wiki_df=df, \n",
    "    spreadsheet=spreadsheet,\n",
    "    sel_cols=['Status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccc524-72ce-4db7-aed5-058c605e044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_status['Match status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6ff28-fc74-4f4a-8f68-5ffbf701bc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_status['sel_col - spreadsheet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760eb885-c43e-4a7e-9be5-b4f5a073b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_first_word = results_status['sel_col - wiki'].str.split(' ').str[0]\n",
    "spread_first_word = results_status['sel_col - spreadsheet'].str.split(' ').str[0]\n",
    "results_status['first word compare'] = wiki_first_word.str.lower()==spread_first_word.str.lower()\n",
    "results_status['first word compare'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765bde5-eaf1-49b7-8a60-f2bd7f7a2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_status[\n",
    "    (results_status['sel_col - spreadsheet'].isna()==False)\n",
    "    & (results_status['Match status'].isin(['mismatch', 'close match']))\n",
    "    & (results_status['first word compare']!=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f036ed-0e19-46c5-91d9-a446c673fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_sel = results_status[\n",
    "    (results_status['Match status']=='mismatch') &\n",
    "    (results_status['first word compare']==False)\n",
    "].sort_values(by='sel_col - wiki')\n",
    "\n",
    "accepted_statuses_first_word_lower = [\n",
    "    'operating',\n",
    "    'mothballed',\n",
    "    'cancelled',\n",
    "    'retired',\n",
    "    'shelved',\n",
    "    'construction',\n",
    "    'pre-permit development',\n",
    "    'announced',\n",
    "    'permitted',\n",
    "]\n",
    "\n",
    "status_sel[~status_sel['sel_col - wiki'].str.split(' ').str[0].str.lower().isin(accepted_statuses_first_word_lower)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf810a7-a57b-43dd-90c7-5fc43f29611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_spon_par = compare_wiki_and_spreadsheet_plant_level_data(\n",
    "    wiki_df=df, \n",
    "    spreadsheet=spreadsheet,\n",
    "    sel_cols=['Sponsor', 'Parent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde30d52-b3f8-4c54-bd54-25145fa92880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export results\n",
    "# results_spon_par.to_excel(\n",
    "#     gcpt_path + \n",
    "#     f'GCPT wiki vs spreadsheet comparison for China - {save_timestamp}.xlsx', \n",
    "#     index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ba955-0bd1-4e10-908d-7a552ac837fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_check = results_spon_par[results_spon_par['sel_col']=='Parent']\n",
    "sponsor_check = results_spon_par[results_spon_par['sel_col']=='Sponsor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7808371-d656-498b-8ebb-1cd721ee78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_check['Match status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aab6e2-67b5-4549-98d1-9721503ddf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_fuzz = 0\n",
    "high_fuzz = 70\n",
    "\n",
    "test = parent_check[\n",
    "    (parent_check['Match status']=='mismatch') & \n",
    "    (parent_check['Fuzz partial ratio'] >= low_fuzz) & \n",
    "    (parent_check['Fuzz partial ratio'] <= high_fuzz)\n",
    "]\n",
    "\n",
    "missing_values = test[(test['sel_col - wiki']=='') | (test['sel_col - spreadsheet']=='')]\n",
    "print(f\"Number of rows with missing value for either wiki or spreadsheet: {len(missing_values)}\")\n",
    "test = test[~test.index.isin(missing_values.index)]\n",
    "\n",
    "test = test.sort_values(by='sel_col - spreadsheet')\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of rows with data with fuzz partial ratios between {low_fuzz} & {high_fuzz}: {len(test)}\")\n",
    "\n",
    "test.loc[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aba998-4ba9-4e47-bef5-df6f37a86613",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['sel_col - spreadsheet']=='nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64bc7e-c476-4012-ab3a-ddb709f4f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a case that's fine, but it looked wrong:\n",
    "# For CKI Lamma power station, Parents:\n",
    "# wiki: Power Assets Holdings, Cheung Kong Infrastructure, CK Hutchison Holdings\n",
    "# spreadsheet: Cheung Kong Infrastructure, CK Hutchison Holdings, Power Assets Holdings\n",
    "# some same info, different order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d475208-1529-4d93-ac8e-7c11f9c0a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sponsor_check['Match status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8a54e-f7e9-4536-958d-143f3e6971d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df[results_df['Match status']=='plant not found in spreadsheet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2327ffc-3326-4ac0-9d00-6c9b32cca0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki title CITIC Zhengzhou power station: \n",
    "# possibly the same as spreadsheet L100725 (Zhengzhou Xinli power station, other names: Henan Xinli power station, Zhengzhou power station)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f00f54-0571-421d-b418-ed519ae41e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: for now, exclude cases with more than one sponsor for a plant (either in the spreadsheet or the wiki)\n",
    "# Those cases will take extra attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fc6b44-260a-4136-ad37-c14996cef7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sponsor_wiki = results_df[results_df['Wiki sponsor num'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6770617-4e4d-4b17-b106-20b14d687ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_sponsor_counts = spreadsheet[['Wiki page', 'Sponsor']].drop_duplicates().groupby('Wiki page')['Sponsor'].count()\n",
    "multi_sponsor_sheet = spreadsheet_sponsor_counts[spreadsheet_sponsor_counts > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191747a0-5aef-4b98-838a-9d827aa639a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_sponsor_sheet.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0596d301-f348-4393-878a-83203e3b85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_wiki = 'https://www.gem.wiki/Banten_Suralaya_power_station'\n",
    "spreadsheet[spreadsheet['Wiki page']==sel_wiki][['Plant', 'Unit', 'Sponsor']].sort_values(by='Unit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c80b8-3e1b-4c73-ac32-bb1a7a4fcfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "# Yuzhnouralsk power station: \n",
    "# Looks like sponsors counted up based on dragging to fill cells.\n",
    "# But also see wiki. Not sure they should all be OGK-3.\n",
    "# There's also GRES-2 Expansion (cancelled), with another sponsor. But this doesn't seem to be in the spreadsheet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041d453-07af-495a-80db-b871742525e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df['Wiki title']=='Hebi Fenghe power station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0064e-7b7b-4e53-a62b-59eea48b019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[(results_df['Fuzz partial ratio'] < 90) & \n",
    "           (results_df['Fuzz partial ratio'] > 80)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177839e2-997b-4523-9fc0-e0eedf669300",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet[spreadsheet['Plant']=='Anhui Bengbu power station']['Sponsor'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb41567-0df5-4ae4-a45f-4915fa3c6fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['Match status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e457a-d24d-4062-ab2e-ddb8c7b6cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87f06ae-dfc0-42ec-8276-d1ce53d6893f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3e64c-ef52-46dc-9719-1e3c7b136030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4d23f-c425-4867-9d8b-e23bb1685dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_values = df[df['category']=='Type']['value']\n",
    "type_values = type_values.str.replace('[', '').str.replace(']', '')\n",
    "type_values = type_values.str.lower()\n",
    "type_values.value_counts().head(30)\n",
    "\n",
    "# TO DO: parse, splitting on '),' and ');'\n",
    "# Then take units sections, generate lines for each unit (to compare against spreadsheet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3dc079-2708-4eab-a5d8-078fc78f99a2",
   "metadata": {},
   "source": [
    "## Existing coal plants in China:\n",
    "* Anhui Hwasu power station has: Plant Details & Plant Details of expansion\n",
    "* Bengbu City power station: like the Baima power station page, but not labeled as a stub\n",
    "* Binzhou Heating Supply Center power station: Has section \"Details of Heating Supply Center power station\" with the usual Project Details stuff\n",
    "* Zhejiang Jiaxing Ultra-supercritical Power Generation Project: Has a fair amount of information, but no project details (or equivalent) section\n",
    "* Huadian Huangjiaozhuang power station: Has some info, but no project details (or equivalent) section\n",
    "* Neijiang Baima power station: has section \"Unit 4 & 5 Project Details\"\n",
    "* Huaneng Dalian power station: Has some info, but no project details (or equivalent) section\n",
    "* Huaneng Xilinhot power station: Has some info, but no project details (or equivalent) section\n",
    "* Shanqiu Fengyuan Aluminum power station: had section \"Plantt Details\"; edited page to fix typo\n",
    "* Baima power station: stub\n",
    "* Dezhou power station: stub\n",
    "* Green Energy power station: stub\n",
    "* Tuoketuo-1 power station: stub\n",
    "* Yuzhou power station: stub\n",
    "* Huangen Fuzhou power station: stub\n",
    "* Qinbei power station: stub\n",
    "* Qitaihe power station: stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c308be0-3981-4cd8-80ab-8d03cedf5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].str.contains('capacity')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462c587b-987f-4f9b-a562-16fdac650a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1e0d7a3-7419-4cb7-9e43-95b6a787babb",
   "metadata": {},
   "source": [
    "# TO DO: \n",
    "* for each page, get the line number for Project Details section\n",
    "  * will look like: \"\\n==Project Details==\\n\"\n",
    "* then get line number for next section:\n",
    "  * will look like: \"\\n==\"\n",
    "* for just the Project Details section:\n",
    "  * parse to get a list of all the row names: anything that starts with *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229594e-525f-49d6-b9dd-8df0bcf5c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data:\n",
    "    if line.startswith('    <title>'):\n",
    "        # create new list to add to\n",
    "        page_list = []\n",
    "        \n",
    "        # put everything into the list until hitting another line that starts with '    <title>'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b3822-88be-4196-b990-8e7fc9753d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
